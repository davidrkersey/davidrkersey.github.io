
</br>
</br>

<center> ![](C:\Users\Griffin\Documents\SpringClasses2020\ML II\Week 3\WMlogoletters.png) </center>


</br>  
</br>



## Applied Exercise - Chapter 7, Q. 6 {.tabset .tabset-fade} 


------------------------------------------------------

###### Team 13: Jasmine Yu, Jianqi chai, David Kersey, Griffin Salyer
* [Return to Homepage](https://griffinsalyer.github.io/team13tp1.github.io/)

### Exercise 6a

</br></br>

6. In this exercise, you will further analyze the Wage data set considered throughout this chapter.
(a) Perform polynomial regression to predict wage using age. Use cross-validation to select the optimal degree d for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial ﬁt to the data.

</br></br>


Set the seed and initialize our packages. We need the boot package for Cross-Validation.
```{r, results="hide"}
set.seed(1)
library(ISLR)
library(boot)
```

</br></br>

perform a 10-fold cross validation. We perform this step in order to determine the optimal degree *d* for the polynomial
```{r, collapse=T, comment=NA, warning=F}
all.deltas = rep(NA, 10)
for (i in 1:10) {
  glm.fit = glm(wage~poly(age, i), data=Wage)
  all.deltas[i] = cv.glm(Wage, glm.fit, K=10)$delta[2]
}
```
</br></br>
Now lets look at the index where we have the minimum error. We find that the lowest error is at `9` - this is the degree at which we will be performing the best polynomial regression.
```{r, collapse=T, comment=NA, warning=F}
which.min(all.deltas)
```
</br></br>
Now lets look at a plot of the errors
```{r}
plot(1:10, all.deltas, xlab="Degree", ylab="CV error", type="l", pch=20, lwd=2, ylim=c(1590, 1700))
min.point = min(all.deltas)
sd.points = sd(all.deltas)
abline(h=min.point + 0.2 * sd.points, col="red", lty="dashed")
abline(h=min.point - 0.2 * sd.points, col="red", lty="dashed")
legend("topright", "0.2-standard deviation lines", lty="dashed", col="red")
```
</br></br></br>

Another way to discover the best degree is to perform an `anova()` to test which degree *d* is best - the results suggest the same as the CV approach.The p-value is indicating whether some polynomial degree is better than the immediately previous polynomial degree. For example, there is a significant improvement from degree `1` to `2`, however, between degree 4 and 5 there is no indicationg that there is a significant improvement in the fit. In our case there are 3 choices - the CV suggests a degree of `9`, however, it is very important to note here that this is not an absolute solution. In terms of parsimony, the `anova()` suggests that degree `4` or `3` and degree `9` are not that different and in this case we should really consider a degree `4` or `3` polynomial regression over a degree `9`. A simpler model is better in this case and gives us roughly the same results. 

</br>
```{r, collapse=T, comment=NA, warning=F}
fit.1 = lm(wage~poly(age, 1), data=Wage)
fit.2 = lm(wage~poly(age, 2), data=Wage)
fit.3 = lm(wage~poly(age, 3), data=Wage)
fit.4 = lm(wage~poly(age, 4), data=Wage)
fit.5 = lm(wage~poly(age, 5), data=Wage)
fit.6 = lm(wage~poly(age, 6), data=Wage)
fit.7 = lm(wage~poly(age, 7), data=Wage)
fit.8 = lm(wage~poly(age, 8), data=Wage)
fit.9 = lm(wage~poly(age, 9), data=Wage)
fit.10 = lm(wage~poly(age, 10), data=Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5, fit.6, fit.7, fit.8, fit.9, fit.10)
```
</br></br>

The code below produces a graph of the data using degree `3` and degree `4` suggested by the `anova()`. Degree `3` is the solid blue line, and degree `4` is the solid red line. Degree `9` is the solid black line.
```{r, collapse=T, comment=NA, warning=F}
plot(wage~age, data=Wage, col="darkgrey")
agelims = range(Wage$age)
age.grid = seq(from=agelims[1], to=agelims[2])

lm.fitd3 = lm(wage~poly(age, 3), data=Wage)
lm.fitd4 = lm(wage~poly(age, 4), data=Wage)

lm.predd3 = predict(lm.fitd3, data.frame(age=age.grid))
lm.predd4 = predict(lm.fitd4, data.frame(age=age.grid))
lines(age.grid, lm.predd3, col="blue", lwd=2)
lines(age.grid, lm.predd4, col="red", lwd=2)
```
</br></br>

And now we see why we really shouldn't use degree 9 - it does not really improve our insight much, if at all, and only complicates our model.
```{r}
plot(wage~age, data=Wage, col="darkgrey")
agelims = range(Wage$age)
age.grid = seq(from=agelims[1], to=agelims[2])

lm.fitd9 = lm(wage~poly(age, 9), data=Wage)
lm.predd9 = predict(lm.fitd9, data.frame(age=age.grid))
lines(age.grid, lm.predd9, col="black", lwd=2)
```
</br>

</br></br></br>

### Exercise 6b

</br></br>

6. In this exercise, you will further analyze the Wage data set considered throughout this chapter.
(b) Fit a step function to predict wage using age, and perform crossvalidation to choose the optimal number of cuts. Make a plot of the ﬁt obtained.

</br></br>

Again, performing a 10-fold CV to choose the optimal number of cuts
```{r, collapse=T, comment=NA, warning=F}
all.cvs = rep(NA, 10)
for (i in 2:10) {
  Wage$age.cut = cut(Wage$age, i)
  lm.fit = glm(wage~age.cut, data=Wage)
  all.cvs[i] = cv.glm(Wage, lm.fit, K=10)$delta[2]
}
```
</br></br>
Let look at the plot and try to determine the best number of cuts.
```{r, collapse=T, comment=NA, warning=F}
plot(2:10, all.cvs[-1], xlab="Number of cuts", ylab="CV error", type="l", pch=20, lwd=2)
```
</br>
The best number of cuts is very obviously 8 - where we have the lowest CV error
</br></br>
Now lets perform a step function (we use glm here instead of lm - its the same functionally)
```{r}
lm.fit = glm(wage~cut(age, 8), data=Wage)
summary(lm.fit)$coef
```
</br></br>

Now we just predict using our step function model, and plot.
```{r}
agelims = range(Wage$age)
age.grid = seq(from=agelims[1], to=agelims[2])
lm.pred = predict(lm.fit, data.frame(age=age.grid))
```

```{r}
plot(wage~age, data=Wage, col="darkgrey")
lines(age.grid, lm.pred, col="red", lwd=2)
```

